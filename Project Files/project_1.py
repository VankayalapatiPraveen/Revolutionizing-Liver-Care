# -*- coding: utf-8 -*-
"""PROJECT 1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LxdiUzJ2URcMop2wKOcWQ7GxBCuVA1Ww
"""

#Reading CSV file
import pandas as pd
dataset = pd.read_excel('/content/HealthCareData.xlsx')

dataset.head()



dataset.shape

dataset.isnull().any

dataset.isnull().sum()

import numpy as np
categorical_features = dataset.select_dtypes(include=[object])
categorical_features.columns

import matplotlib.pyplot as plt
import seaborn as sns

c=0

plt.figure(figsize=(20,15))

for i in dataset.columns:
    # Attempt to convert the column to numeric, coercing errors
    numeric_col = pd.to_numeric(dataset[i], errors='coerce')
    # Check if the column is numeric after conversion and has more than one unique value
    if pd.api.types.is_numeric_dtype(numeric_col) and numeric_col.nunique() > 1:
        plt.subplot(7,5,c+1)
        sns.boxplot(x=numeric_col)
        plt.title(i)
        c+=1

plt.tight_layout()
plt.show()

q1 = dataset['Eosinophils   (%)'].quantile(0.25)

q3 = dataset['Eosinophils   (%)'].quantile(0.75)

iqr = q3 - q1

q1, q3, iqr

upper_limit = q3 + (1.5 * iqr)

lower_limit = q1 - (1.5 * iqr)

lower_limit, upper_limit

dataset['Eosinophils   (%)'] = np.where(dataset['Eosinophils   (%)'] > upper_limit,
                                     upper_limit,
                                     np.where(dataset['Eosinophils   (%)'] < lower_limit, lower_limit, dataset['Eosinophils   (%)']))

sns.boxplot(dataset['Eosinophils   (%)'])

sns.boxplot(dataset['Basophils  (%)'])

dataset.columns

q1= dataset['Basophils  (%)'].quantile(0.25)

q3= dataset['Basophils  (%)'].quantile(0.75)

iqr = q3 - q1

q1,q3, iqr

upper_limit = q3+ (1.5*iqr)

lower_limit = q1 - (1.5*iqr)

lower_limit, upper_limit

dataset['Basophils  (%)'] = np.where(dataset['Basophils  (%)'] > upper_limit, upper_limit,

np.where(dataset['Basophils  (%)'] < lower_limit, lower_limit, dataset['Basophils  (%)']))

sns.boxplot(dataset['Basophils  (%)'])

sns.boxplot(dataset['Platelet Count  (lakhs/mm)'])

y = dataset[ 'Predicted Value(Out Come-Patient suffering from liver  cirrosis or not)' ]
x = dataset.drop('Predicted Value(Out Come-Patient suffering from liver  cirrosis or not)', axis=1)
from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.2,random_state=42)

x_train

x_test

y_train

y_test

dataset.describe()

sns.boxplot(x='Age', y='Diabetes Result', data=dataset, hue='Gender')

plt.title('Gender vs Diabetes Result', color='red', size=20)

plt.show()

sns.barplot(x=dataset['Place(location where the patient lives)'],y=dataset['Age'])

sns.boxplot(x='Place(location where the patient lives)', y='Age', data=dataset)

plt.title('Place vs Age', color='red', size=20)

plt.show()



sns.countplot(data=dataset,x='Place(location where the patient lives)')

plt.title("Location", color='y', size=20, loc='left')

plt.show()

plt.figure(figsize=(15,10))

sns.heatmap(dataset.corr(numeric_only=True),annot=True)

plt.show()

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.2,random_state=42)

x_train

x_test

y_train

y_test

# Drop rows with any missing values from the training data
x_train_cleaned = x_train.dropna()
y_train_cleaned = y_train[x_train_cleaned.index]

# Drop rows with any missing values from the test data
x_test_cleaned = x_test.dropna()
y_test_cleaned = y_test[x_test_cleaned.index]

print("Shape of original training data:", x_train.shape)
print("Shape of cleaned training data:", x_train_cleaned.shape)
print("Shape of original test data:", x_test.shape)
print("Shape of cleaned test data:", x_test_cleaned.shape)

x_train

y_train

"""**Reasoning**:
Convert 'A/G Ratio' and 'LDL' columns to numeric, coercing errors to NaN.


"""

dataset['A/G Ratio'] = pd.to_numeric(dataset['A/G Ratio'], errors='coerce')
dataset['LDL'] = pd.to_numeric(dataset['LDL'], errors='coerce')

from sklearn.naive_bayes import GaussianNB

nb = GaussianNB()

# Note: x_train and y_train must be preprocessed (numeric, no NaNs, encoded) before fitting.
# Based on previous attempts, your data still needs cleaning and encoding.

# Assuming data is preprocessed:
# nb.fit(x_train_processed,y_train_processed)

# print ('train_score '+ str(nb.score(x_train_processed,y_train_processed)))
# print ('test_score '+ str(nb.score(x_test_processed,y_test_processed)))

print("Syntax errors fixed. Please ensure your data is preprocessed before running the fit and score lines.")

"""**Preprocess the data**: Handle missing values and encode categorical features in the training and testing datasets."""

# Identify numerical and categorical features
# Exclude 'A/G Ratio' from numerical features due to inconsistent data types
numerical_features = x_train.select_dtypes(include=np.number).columns.tolist()
if 'A/G Ratio' in numerical_features:
    numerical_features.remove('A/G Ratio')

categorical_features = x_train.select_dtypes(include='object').columns.tolist()
# Add 'A/G Ratio' to categorical features if it was originally an object type
if 'A/G Ratio' not in numerical_features and 'A/G Ratio' not in categorical_features and 'A/G Ratio' in x_train.columns:
    categorical_features.append('A/G Ratio')


# Impute missing values in numerical features using the mean
from sklearn.impute import SimpleImputer
imputer_numerical = SimpleImputer(strategy='mean')
x_train_numerical_imputed = imputer_numerical.fit_transform(x_train[numerical_features])
x_test_numerical_imputed = imputer_numerical.transform(x_test[numerical_features])

# Impute missing values in categorical features using the most frequent value
imputer_categorical = SimpleImputer(strategy='most_frequent')
x_train_categorical_imputed = imputer_categorical.fit_transform(x_train[categorical_features])
x_test_categorical_imputed = imputer_categorical.transform(x_test[categorical_features])

# Convert imputed arrays back to DataFrames to maintain column names
x_train_numerical_imputed_df = pd.DataFrame(x_train_numerical_imputed, columns=numerical_features, index=x_train.index)
x_test_numerical_imputed_df = pd.DataFrame(x_test_numerical_imputed, columns=numerical_features, index=x_test.index)
x_train_categorical_imputed_df = pd.DataFrame(x_train_categorical_imputed, columns=categorical_features, index=x_train.index)
x_test_categorical_imputed_df = pd.DataFrame(x_test_categorical_imputed, columns=categorical_features, index=x_test.index)

# Convert categorical columns to string type before encoding
x_train_categorical_imputed_df = x_train_categorical_imputed_df.astype(str)
x_test_categorical_imputed_df = x_test_categorical_imputed_df.astype(str)

# Encode categorical features using One-Hot Encoding
from sklearn.preprocessing import OneHotEncoder

# Initialize OneHotEncoder, handling unknown categories and dropping the first category to avoid multicollinearity
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='first')

# Fit and transform on training data
x_train_categorical_encoded = encoder.fit_transform(x_train_categorical_imputed_df)
# Transform on test data
x_test_categorical_encoded = encoder.transform(x_test_categorical_imputed_df)

# Convert encoded arrays back to DataFrames
x_train_categorical_encoded_df = pd.DataFrame(x_train_categorical_encoded, columns=encoder.get_feature_names_out(categorical_features), index=x_train.index)
x_test_categorical_encoded_df = pd.DataFrame(x_test_categorical_encoded, columns=encoder.get_feature_names_out(categorical_features), index=x_test.index)


# Concatenate numerical and encoded categorical features
x_train_processed = pd.concat([x_train_numerical_imputed_df, x_train_categorical_encoded_df], axis=1)
x_test_processed = pd.concat([x_test_numerical_imputed_df, x_test_categorical_encoded_df], axis=1)

# Preprocess the target variable y (if necessary, e.g., label encoding for classification)
# Impute missing values in the target variable using the most frequent value
imputer_y = SimpleImputer(strategy='most_frequent')
y_train_imputed = imputer_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()
y_test_imputed = imputer_y.transform(y_test.values.reshape(-1, 1)).ravel()

# Encode the target variable using Label Encoding
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_train_processed = label_encoder.fit_transform(y_train_imputed)
y_test_processed = label_encoder.transform(y_test_imputed)


# Display the shapes of the processed dataframes
print("Shape of x_train_processed:", x_train_processed.shape)
print("Shape of x_test_processed:", x_test_processed.shape)
print("Shape of y_train_processed:", y_train_processed.shape)
print("Shape of y_test_processed:", y_test_processed.shape)

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier()

# rf.fit(x_train,y_train)

# print ('train_score '+ str(rf.score(x_train,y_train)))
# print ('test_score '+ str(rf.score(x_test,y_test)))

print("Syntax errors fixed. Please ensure your data is preprocessed before running the fit and score lines.")

"""**Train the Random Forest model**: Train the `RandomForestClassifier` model using the preprocessed training data."""

# Train the Random Forest model
# Ensure that x_train_processed and y_train_processed are defined and preprocessed

# Check for missing values right before fitting
print("Checking for missing values in x_train_processed before fitting:")
print(x_train_processed.isnull().sum().sum())
print("\nChecking for missing values in y_train_processed before fitting:")
# Assuming y_train_processed is a numpy array after Label Encoding
print(np.isnan(y_train_processed).sum())


rf.fit(x_train_processed, y_train_processed)

print("Random Forest model trained.")

"""**Evaluate the Random Forest model**: Predict on the preprocessed test data and evaluate the model's performance."""

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Predict on the test data
# Ensure that x_test_processed is defined and preprocessed
y_pred_rf = rf.predict(x_test_processed)

# Evaluate the model
# Ensure that y_test_processed and y_pred_rf are defined
print("Random Forest Model Performance:")
print("Accuracy:", accuracy_score(y_test_processed, y_pred_rf))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test_processed, y_pred_rf))
print("\nClassification Report:")
print(classification_report(y_test_processed, y_pred_rf))

# This cell trains the Random Forest model using the preprocessed data.
# Ensure that x_train_processed and y_train_processed are defined from the previous preprocessing step.

# The code from cell d06bb7bc is already in the notebook and was modified.
# You can now run that cell to train the model.
print("Executing cell d06bb7bc to train the Random Forest model.")

# Check for missing values in the processed training features
print("Missing values in x_train_processed:")
print(x_train_processed.isnull().sum().sum())

# Check for missing values in the processed training target
# This check is removed as the target variable is now numerically encoded.
# print("\nMissing values in y_train_processed:")
# print(np.isnan(y_train_processed).sum())

# This cell trains the Random Forest model using the preprocessed data.
# Ensure that x_train_processed and y_train_processed are defined and free of NaNs.

# The code from cell d06bb7bc is already in the notebook and was modified to be runnable.
# You can now run that cell to train the model.
print("Executing cell d06bb7bc to train the Random Forest model.")

# This cell evaluates the trained Random Forest model.
# Ensure that x_test_processed, y_test_processed, and the trained 'rf' model are defined.

# The code from cell f9bd80f9 is already in the notebook and was modified to be runnable.
# You can now run that cell to evaluate the model.
print("Executing cell f9bd80f9 to evaluate the Random Forest model.")

# Combine training and evaluation to ensure correct execution order

# Code from cell d06bb7bc (Train the Random Forest model)
print("Training the Random Forest model...")
# Ensure that x_train_processed and y_train_processed are defined and free of NaNs.

# Check for missing values right before fitting
print("Checking for missing values in x_train_processed before fitting:")
print(x_train_processed.isnull().sum().sum())
print("\nChecking for missing values in y_train_processed before fitting:")
# Assuming y_train_processed is a numpy array after Label Encoding
print(np.isnan(y_train_processed).sum())

# Assuming rf is already initialized
rf.fit(x_train_processed, y_train_processed)
print("Random Forest model trained.")

print("-" * 30) # Separator

# Code from cell f9bd80f9 (Evaluate the Random Forest model)
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

print("Evaluating the Random Forest model...")
# Predict on the test data
# Ensure that x_test_processed is defined and preprocessed
y_pred_rf = rf.predict(x_test_processed)

# Evaluate the model
# Ensure that y_test_processed and y_pred_rf are defined
print("Random Forest Model Performance:")
print("Accuracy:", accuracy_score(y_test_processed, y_pred_rf))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test_processed, y_pred_rf))
print("\nClassification Report:")
print(classification_report(y_test_processed, y_pred_rf))

from sklearn.linear_model import LogisticRegression

log = LogisticRegression()

# Note: x_train and y_train must be preprocessed (numeric, no NaNs, encoded) before fitting.
# Based on previous attempts, your data still needs cleaning and encoding.

# Assuming data is preprocessed:
# logistic = log.fit(x_train_processed,y_train_processed)

# print ('train_score '+ str(log.score(x_train_processed,y_train_processed)))
# print ('test_score '+ str(log.score(x_test_processed,y_test_processed)))

print("Syntax errors fixed. Please ensure your data is preprocessed before running the fit and score lines.")

"""**Train the Logistic Regression model**: Train the `LogisticRegression` model using the preprocessed training data."""

# Train the Logistic Regression model
# Ensure that x_train_processed and y_train_processed are defined and preprocessed
log.fit(x_train_processed, y_train_processed)

print("Logistic Regression model trained.")

"""**Evaluate the Logistic Regression model**: Predict on the preprocessed test data and evaluate the model's performance."""

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Predict on the test data
# Ensure that x_test_processed is defined and preprocessed
y_pred_log = log.predict(x_test_processed)

# Evaluate the model
# Ensure that y_test_processed and y_pred_log are defined
print("Logistic Regression Model Performance:")
print("Accuracy:", accuracy_score(y_test_processed, y_pred_log))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test_processed, y_pred_log))
print("\nClassification Report:")
print(classification_report(y_test_processed, y_pred_log))

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier()

# Note: x_train and y_train must be preprocessed (numeric, no NaNs, encoded) before fitting.
# Based on previous attempts, your data still needs cleaning and encoding.

# Assuming data is preprocessed:
# knn.fit(x_train_processed,y_train_processed)

# print ('train_score '+ str(knn.score(x_train_processed,y_train_processed)))
# print ('test_score '+ str(knn.score(x_test_processed,y_test_processed)))

print("Syntax errors fixed. Please ensure your data is preprocessed before running the fit and score lines.")

"""**Train the K-Nearest Neighbors model**: Train the `KNeighborsClassifier` model using the preprocessed training data."""

# Train the K-Nearest Neighbors model
# Ensure that x_train_processed and y_train_processed are defined and preprocessed
knn.fit(x_train_processed, y_train_processed)

print("K-Nearest Neighbors model trained.")

"""**Evaluate the K-Nearest Neighbors model**: Predict on the preprocessed test data and evaluate the model's performance."""

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Predict on the test data
# Ensure that x_test_processed is defined and preprocessed
y_pred_knn = knn.predict(x_test_processed)

# Evaluate the model
# Ensure that y_test_processed and y_pred_knn are defined
print("K-Nearest Neighbors Model Performance:")
print("Accuracy:", accuracy_score(y_test_processed, y_pred_knn))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test_processed, y_pred_knn))
print("\nClassification Report:")
print(classification_report(y_test_processed, y_pred_knn))

from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV

k = np.random.randint(1,50,60)

params = {'n_neighbors': k}

random_search = RandomizedSearchCV(knn, params,n_iter=5,cv=5,n_jobs=-1, verbose=0)
random_search.fit(x_train_processed,y_train_processed)

print ('train_score '+ str(random_search.score(x_train_processed,y_train_processed)))
print ('test_score '+ str(random_search.score(x_test_processed,y_test_processed)))